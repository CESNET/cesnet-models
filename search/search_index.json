{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CESNET Models","text":"<p>This is the documentation of the CESNET Models project. </p> <p>The goal of this project is to provide pre-trained neural networks for traffic classification in a similar fashion to what torchvision is doing for the computer vision field.</p> <p>Models from the following papers are included:</p> <ul> <li> <p>Fine-grained TLS services classification with reject option  Jan Luxemburk and Tom\u00e1\u0161 \u010cejka  Computer Networks</p> </li> <li> <p>Encrypted traffic classification: the QUIC case  Jan Luxemburk and Karel Hynek  2023 7th Network Traffic Measurement and Analysis Conference (TMA)</p> </li> </ul>"},{"location":"getting_started/","title":"Getting started","text":""},{"location":"getting_started/#jupyter-notebooks","title":"Jupyter notebooks","text":"<p>Example Jupyter notebooks are provided at https://github.com/CESNET/cesnet-tcexamples. Start with:</p> <ul> <li>Evaluate a pre-trained neural network I (TLS) - reproduce_tls.ipynb</li> <li>Evaluate a pre-trained neural network II (QUIC) - reproduce_quic.ipynb</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Install the package from pip with:</p> <pre><code>pip install cesnet-models\n</code></pre> <p>or for editable install with:</p> <pre><code>pip install -e git+https://github.com/CESNET/cesnet-models\n</code></pre>"},{"location":"installation/#requirements","title":"Requirements","text":"<p>The <code>cesnet-models</code> package requires Python &gt;=3.10.</p>"},{"location":"installation/#dependencies","title":"Dependencies","text":"Name Version numpy scikit-learn torch &gt;=1.10"},{"location":"reference_models/","title":"Available models","text":""},{"location":"reference_models/#models.mm_cesnet_v2","title":"models.mm_cesnet_v2","text":"<pre><code>mm_cesnet_v2(\n    weights=None,\n    model_dir=None,\n    num_classes=None,\n    flowstats_input_size=None,\n    ppi_input_channels=None,\n)\n</code></pre> <p>This is a second version of the multimodal CESNET architecture. It was used in the \"Encrypted traffic classification: the QUIC case\" paper.</p> Changes from the first version <ul> <li>Global pooling was added to the CNN part processing PPI sequences, instead of a simple flattening.</li> <li>One more Conv1d layer was added to the CNN part and the number of channels was increased.</li> <li>The size of the MLP processing flow statistics was increased.</li> <li>The size of the MLP processing shared representations was decreased.</li> <li>Some dropout rates were decreased.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Optional[MM_CESNET_V2_Weights]</code> <p>If provided, the model will be initialized with these weights.</p> <code>None</code> <code>model_dir</code> <code>Optional[str]</code> <p>If weights are provided, this folder will be used to store the weights.</p> <code>None</code> <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes for the classification task.</p> <code>None</code> <code>flowstats_input_size</code> <code>Optional[int]</code> <p>Size of the flow statistics input.</p> <code>None</code> <code>ppi_input_channels</code> <code>Optional[int]</code> <p>Number of channels in the PPI input.</p> <code>None</code> Source code in <code>cesnet_models\\models.py</code> <pre><code>def mm_cesnet_v2(weights: Optional[MM_CESNET_V2_Weights] = None,\n                 model_dir: Optional[str] = None,\n                 num_classes: Optional[int] = None,\n                 flowstats_input_size: Optional[int] = None,\n                 ppi_input_channels: Optional[int] = None,\n                 ) -&gt; Multimodal_CESNET:\n    \"\"\"\n    This is a second version of the multimodal CESNET architecture. It was used in\n    the *\"Encrypted traffic classification: the QUIC case\"* paper.\n\n    Changes from the first version:\n        - Global pooling was added to the CNN part processing PPI sequences, instead of a simple flattening.\n        - One more Conv1d layer was added to the CNN part and the number of channels was increased.\n        - The size of the MLP processing flow statistics was increased.\n        - The size of the MLP processing shared representations was decreased.\n        - Some dropout rates were decreased.\n\n    Parameters:\n        weights: If provided, the model will be initialized with these weights.\n        model_dir: If weights are provided, this folder will be used to store the weights.\n        num_classes: Number of classes for the classification task.\n        flowstats_input_size: Size of the flow statistics input.\n        ppi_input_channels: Number of channels in the PPI input.\n    \"\"\"\n    v2_model_configuration = {\n        \"conv_normalization\": NormalizationEnum.BATCH_NORM,\n        \"linear_normalization\": NormalizationEnum.BATCH_NORM,\n        \"cnn_num_hidden\": 3,\n        \"cnn_channels1\": 200,\n        \"cnn_channels2\": 300,\n        \"cnn_channels3\": 300,\n        \"cnn_use_pooling\": True,\n        \"cnn_dropout_rate\": 0.1,\n        \"flowstats_num_hidden\": 2,\n        \"flowstats_size\": 225,\n        \"flowstats_out_size\": 225,\n        \"flowstats_dropout_rate\": 0.1,\n        \"latent_num_hidden\":  0,\n        \"latent_size\": 600,\n        \"latent_dropout_rate\": 0.2,\n    }\n    return _multimodal_cesnet(model_configuration=v2_model_configuration,\n                              weights=weights,\n                              model_dir=model_dir,\n                              num_classes=num_classes,\n                              flowstats_input_size=flowstats_input_size,\n                              ppi_input_channels=ppi_input_channels)\n</code></pre>"},{"location":"reference_models/#models.mm_cesnet_v1","title":"models.mm_cesnet_v1","text":"<pre><code>mm_cesnet_v1(\n    weights=None,\n    model_dir=None,\n    num_classes=None,\n    flowstats_input_size=None,\n    ppi_input_channels=None,\n)\n</code></pre> <p>This model was used in the \"Fine-grained TLS services classification with reject option\" paper.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>Optional[MM_CESNET_V1_Weights]</code> <p>If provided, the model will be initialized with these weights.</p> <code>None</code> <code>model_dir</code> <code>Optional[str]</code> <p>If weights are provided, this folder will be used to store the weights.</p> <code>None</code> <code>num_classes</code> <code>Optional[int]</code> <p>Number of classes for the classification task.</p> <code>None</code> <code>flowstats_input_size</code> <code>Optional[int]</code> <p>Size of the flow statistics input.</p> <code>None</code> <code>ppi_input_channels</code> <code>Optional[int]</code> <p>Number of channels in the PPI input.</p> <code>None</code> Source code in <code>cesnet_models\\models.py</code> <pre><code>def mm_cesnet_v1(weights: Optional[MM_CESNET_V1_Weights] = None,\n                 model_dir: Optional[str] = None,\n                 num_classes: Optional[int] = None,\n                 flowstats_input_size: Optional[int] = None,\n                 ppi_input_channels: Optional[int] = None,\n                 ) -&gt; Multimodal_CESNET:\n    \"\"\"\n    This model was used in the *\"Fine-grained TLS services classification with reject option\"* paper.\n\n    Parameters:\n        weights: If provided, the model will be initialized with these weights.\n        model_dir: If weights are provided, this folder will be used to store the weights.\n        num_classes: Number of classes for the classification task.\n        flowstats_input_size: Size of the flow statistics input.\n        ppi_input_channels: Number of channels in the PPI input.\n    \"\"\"\n    v1_model_configuration = {\n        \"conv_normalization\": NormalizationEnum.BATCH_NORM,\n        \"linear_normalization\": NormalizationEnum.BATCH_NORM,\n        \"cnn_num_hidden\": 2,\n        \"cnn_channels1\": 72,\n        \"cnn_channels2\": 128,\n        \"cnn_channels3\": 128,\n        \"cnn_use_pooling\": False,\n        \"cnn_dropout_rate\": 0.2,\n        \"flowstats_num_hidden\": 2,\n        \"flowstats_size\": 64,\n        \"flowstats_out_size\": 32,\n        \"flowstats_dropout_rate\": 0.2,\n        \"latent_num_hidden\": 1,\n        \"latent_size\": 480,\n        \"latent_dropout_rate\": 0.2,\n    }\n    return _multimodal_cesnet(model_configuration=v1_model_configuration,\n                              weights=weights,\n                              model_dir=model_dir,\n                              num_classes=num_classes,\n                              flowstats_input_size=flowstats_input_size,\n                              ppi_input_channels=ppi_input_channels)\n</code></pre>"},{"location":"reference_transforms/","title":"Data transformations","text":"<p>Available data transformations.</p>"},{"location":"reference_transforms/#transforms.ClipAndScalePPI","title":"transforms.ClipAndScalePPI","text":"<p>             Bases: <code>Module</code></p> <p>Transform class for scaling of per-packet information (PPI) sequences. This transform clips packet sizes and inter-packet times and scales them using a specified scaler. This class inherits from <code>nn.Module</code>, and the data transformation is implemented in the <code>forward</code> method.</p> <p>When used with the CESNET DataZoo package, the transform will be fitted during dataset initialization. Otherwise, the <code>psizes_scaler_attrs</code> and <code>ipt_scaler_attrs</code> must be provided. The required entries in <code>psizes_scaler_attrs</code> and <code>ipt_scaler_attrs</code> depend on the scaler used.</p> <ul> <li>For <code>StandardScaler</code>, the required attributes are <code>mean_</code> and <code>scale_</code>.</li> <li>For <code>RobustScaler</code>, the required attributes are <code>center_</code> and <code>scale_</code>.</li> <li>For <code>MinMaxScaler</code>,  the required attributes <code>min_</code> and <code>scale_</code>.</li> </ul> <p>Expected format of input PPI sequences: <code>(batch_size, ppi_length, ppi_channels)</code></p> <p>Info</p> <p>The zero padding in PPI sequences is preserved during scaling, i.e., the padding zeroes are kept in the output.</p> <p>Parameters:</p> Name Type Description Default <code>psizes_scaler_enum</code> <code>ScalerEnum | str</code> <p>What scaler should be used for packet sizes. Options are standard, robust, and minmax.</p> <code>STANDARD</code> <code>ipt_scaler_enum</code> <code>ScalerEnum | str</code> <p>What scaler should be used for inter-packet times. Options are standard, robust, and minmax.</p> <code>STANDARD</code> <code>pszies_min</code> <code>int</code> <p>Clip packet sizes to this minimum value.</p> <code>1</code> <code>psizes_max</code> <code>int</code> <p>Clip packet sizes to this maximum value.</p> <code>1500</code> <code>ipt_min</code> <code>int</code> <p>Clip inter-packet times to this minimum value.</p> <code>0</code> <code>ipt_max</code> <code>int</code> <p>Clip inter-packet times to this maximum value.</p> <code>65000</code> <code>psizes_scaler_attrs</code> <code>Optional[dict[str, list[float]]]</code> <p>To use a pre-fitted packet sizes scaler, provide its attributes here.</p> <code>None</code> <code>ipt_scaler_attrs</code> <code>Optional[dict[str, list[float]]]</code> <p>To use a pre-fitted inter-packet times scaler, provide its attributes here.</p> <code>None</code> Source code in <code>cesnet_models\\transforms.py</code> <pre><code>class ClipAndScalePPI(nn.Module):\n    \"\"\"\n    Transform class for scaling of per-packet information (PPI) sequences. This transform clips packet sizes and inter-packet times and scales them using a specified scaler.\n    This class inherits from `nn.Module`, and the data transformation is implemented in the `forward` method.\n\n    When used with the CESNET DataZoo package, the transform will be fitted during dataset initialization. Otherwise, the `psizes_scaler_attrs` and `ipt_scaler_attrs` must be provided.\n    The required entries in `psizes_scaler_attrs` and `ipt_scaler_attrs` depend on the scaler used.\n\n    - For `StandardScaler`, the required attributes are `mean_` and `scale_`.\n    - For `RobustScaler`, the required attributes are `center_` and `scale_`.\n    - For `MinMaxScaler`,  the required attributes `min_` and `scale_`.\n\n    Expected format of input PPI sequences: `(batch_size, ppi_length, ppi_channels)`\n\n    !!! info Padding\n        The zero padding in PPI sequences is preserved during scaling, i.e., the padding zeroes are kept in the output.\n\n    Parameters:\n        psizes_scaler_enum: What scaler should be used for packet sizes. Options are standard, robust, and minmax.\n        ipt_scaler_enum: What scaler should be used for inter-packet times. Options are standard, robust, and minmax.\n        pszies_min: Clip packet sizes to this minimum value.\n        psizes_max: Clip packet sizes to this maximum value.\n        ipt_min: Clip inter-packet times to this minimum value.\n        ipt_max: Clip inter-packet times to this maximum value.\n        psizes_scaler_attrs: To use a pre-fitted packet sizes scaler, provide its attributes here.\n        ipt_scaler_attrs: To use a pre-fitted inter-packet times scaler, provide its attributes here.\n    \"\"\"\n    psizes_scaler: StandardScaler | RobustScaler | MinMaxScaler\n    ipt_scaler: StandardScaler | RobustScaler | MinMaxScaler\n    pszies_min: int\n    psizes_max: int\n    ipt_min: int\n    ipt_max: int\n\n    def __init__(self,\n                 psizes_scaler_enum: ScalerEnum | str = ScalerEnum.STANDARD,\n                 ipt_scaler_enum: ScalerEnum | str = ScalerEnum.STANDARD,\n                 pszies_min: int = 1,\n                 psizes_max: int = 1500,\n                 ipt_min: int = 0,\n                 ipt_max: int = 65000,\n                 psizes_scaler_attrs: Optional[dict[str, list[float]]] = None,\n                 ipt_scaler_attrs: Optional[dict[str, list[float]]] = None) -&gt; None:\n        super().__init__()\n        if psizes_scaler_enum == ScalerEnum.STANDARD:\n            self.psizes_scaler = StandardScaler()\n        elif psizes_scaler_enum == ScalerEnum.ROBUST:\n            self.psizes_scaler = RobustScaler()\n        elif psizes_scaler_enum == ScalerEnum.MINMAX:\n            self.psizes_scaler = MinMaxScaler()\n        else:\n            raise ValueError(f\"psizes_scaler_enum must be one of {ScalerEnum.__members__}\")\n        if ipt_scaler_enum == ScalerEnum.STANDARD:\n            self.ipt_scaler = StandardScaler()\n        elif ipt_scaler_enum == ScalerEnum.ROBUST:\n            self.ipt_scaler = RobustScaler()\n        elif ipt_scaler_enum == ScalerEnum.MINMAX:\n            self.ipt_scaler = MinMaxScaler()\n        else:\n            raise ValueError(f\"ipt_scaler_enum must be one of {ScalerEnum.__members__}\")\n        self.pszies_min = pszies_min\n        self.psizes_max = psizes_max\n        self.ipt_max = ipt_max\n        self.ipt_min = ipt_min\n        self._psizes_scaler_enum = psizes_scaler_enum\n        self._ipt_scaler_enum = ipt_scaler_enum\n\n        if psizes_scaler_attrs is None and ipt_scaler_attrs is None:\n            self.needs_fitting = True\n        elif psizes_scaler_attrs is not None and ipt_scaler_attrs is not None:\n            set_scaler_attrs(scaler=self.psizes_scaler, scaler_attrs=psizes_scaler_attrs)\n            set_scaler_attrs(scaler=self.ipt_scaler, scaler_attrs=ipt_scaler_attrs)\n            self.needs_fitting = False\n        else:\n            raise ValueError(\"psizes_scaler_attrs and ipt_scaler_attrs must be both set or both None\")\n\n    def forward(self, x_ppi: np.ndarray) -&gt; np.ndarray:\n        if self.needs_fitting:\n            raise ValueError(\"Scalers need to be fitted before using the ClipAndScalePPI transform\")\n        x_ppi = x_ppi.transpose(0, 2, 1)\n        orig_shape = x_ppi.shape\n        ppi_channels = x_ppi.shape[-1]\n        x_ppi = x_ppi.reshape(-1, ppi_channels)\n        x_ppi[:, IPT_POS] = x_ppi[:, IPT_POS].clip(max=self.ipt_max, min=self.ipt_min)\n        x_ppi[:, SIZE_POS] = x_ppi[:, SIZE_POS].clip(max=self.psizes_max, min=self.pszies_min)\n        padding_mask = x_ppi[:, DIR_POS] == 0 # Mask of zero padding\n        x_ppi[:, IPT_POS] = self.ipt_scaler.transform(x_ppi[:, IPT_POS].reshape(-1, 1)).reshape(-1) # type: ignore\n        x_ppi[:, SIZE_POS] = self.psizes_scaler.transform(x_ppi[:, SIZE_POS].reshape(-1, 1)).reshape(-1) # type: ignore\n        x_ppi[padding_mask, IPT_POS] = 0\n        x_ppi[padding_mask, SIZE_POS] = 0\n        x_ppi = x_ppi.reshape(orig_shape).transpose(0, 2, 1)\n        return x_ppi\n\n    def to_dict(self) -&gt; dict:\n        d = {\n            \"psizes_scaler_enum\": str(self._psizes_scaler_enum),\n            \"psizes_scaler_attrs\": get_scaler_attrs(self.psizes_scaler),\n            \"pszies_min\": self.pszies_min,\n            \"psizes_max\": self.psizes_max,\n            \"ipt_scaler_enum\": str(self._ipt_scaler_enum),\n            \"ipt_scaler_attrs\": get_scaler_attrs(self.ipt_scaler),\n            \"ipt_min\": self.ipt_min,\n            \"ipt_max\": self.ipt_max,\n        }\n        return d\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(psizes_scaler={self._psizes_scaler_enum}, ipt_scaler={self._ipt_scaler_enum}, pszies_min={self.pszies_min}, psizes_max={self.psizes_max}, ipt_min={self.ipt_min}, ipt_max={self.ipt_max})\"\n</code></pre>"},{"location":"reference_transforms/#transforms.ClipAndScaleFlowstats","title":"transforms.ClipAndScaleFlowstats","text":"<p>             Bases: <code>Module</code></p> <p>Transform class for scaling of features describing an entire network flow -- called flow statistics. This transform clips flow statistics to their <code>quantile_clip</code> quantile and scales them using a specified scaler. This class inherits from <code>nn.Module</code>, and the data transformation is implemented in the <code>forward</code> method.</p> <p>When used with the CESNET DataZoo package, the transform will be fitted during dataset initialization. Otherwise, the <code>flowstats_scaler_attrs</code> must be provided. The required entries in <code>flowstats_scaler_attrs</code> depend on the scaler used.</p> <ul> <li>For <code>StandardScaler</code>, the required attributes are <code>mean_</code> and <code>scale_</code>.</li> <li>For <code>RobustScaler</code>, the required attributes are <code>center_</code> and <code>scale_</code>.</li> <li>For <code>MinMaxScaler</code>,  the required attributes <code>min_</code> and <code>scale_</code>.</li> </ul> <p>Expected format of input flow statistics: <code>(batch_size, flowstats_features)</code></p> <p>Parameters:</p> Name Type Description Default <code>flowstats_scaler_enum</code> <code>ScalerEnum | str</code> <p>What scaler should be used for flow statistics. Options are standard, robust, and minmax.</p> <code>ROBUST</code> <code>quantile_clip</code> <code>float</code> <p>Clip flow statistics to this quantile.</p> <code>0.99</code> <code>flowstats_quantiles</code> <code>Optional[list[float]]</code> <p>To use pre-fitted quantiles, provide them here.</p> <code>None</code> <code>flowstats_scaler_attrs</code> <code>Optional[dict[str, list[float]]]</code> <p>To use a pre-fitted scaler, provide its attributes here.</p> <code>None</code> Source code in <code>cesnet_models\\transforms.py</code> <pre><code>class ClipAndScaleFlowstats(nn.Module):\n    \"\"\"\n    Transform class for scaling of features describing an entire network flow -- called flow statistics. This transform clips flow statistics to their `quantile_clip` quantile and scales them using a specified scaler.\n    This class inherits from `nn.Module`, and the data transformation is implemented in the `forward` method.\n\n    When used with the CESNET DataZoo package, the transform will be fitted during dataset initialization. Otherwise, the `flowstats_scaler_attrs` must be provided.\n    The required entries in `flowstats_scaler_attrs` depend on the scaler used.\n\n    - For `StandardScaler`, the required attributes are `mean_` and `scale_`.\n    - For `RobustScaler`, the required attributes are `center_` and `scale_`.\n    - For `MinMaxScaler`,  the required attributes `min_` and `scale_`.\n\n    Expected format of input flow statistics: `(batch_size, flowstats_features)`\n\n    Parameters:\n        flowstats_scaler_enum: What scaler should be used for flow statistics. Options are standard, robust, and minmax.\n        quantile_clip: Clip flow statistics to this quantile.\n        flowstats_quantiles:  To use pre-fitted quantiles, provide them here.\n        flowstats_scaler_attrs: To use a pre-fitted scaler, provide its attributes here.\n    \"\"\"\n    flowstats_scaler: StandardScaler | RobustScaler | MinMaxScaler\n    quantile_clip: float\n    flowstats_quantiles: Optional[list[float]]\n\n    def __init__(self,\n                 flowstats_scaler_enum: ScalerEnum | str = ScalerEnum.ROBUST,\n                 quantile_clip: float = 0.99,\n                 flowstats_quantiles: Optional[list[float]] = None,\n                 flowstats_scaler_attrs: Optional[dict[str, list[float]]] = None) -&gt; None:\n        super().__init__()\n        if flowstats_scaler_enum == ScalerEnum.STANDARD:\n            self.flowstats_scaler = StandardScaler()\n        elif flowstats_scaler_enum == ScalerEnum.ROBUST:\n            self.flowstats_scaler = RobustScaler()\n        elif flowstats_scaler_enum == ScalerEnum.MINMAX:\n            self.flowstats_scaler = MinMaxScaler()\n        else:\n            raise ValueError(f\"flowstats_scaler_enum must be one of {ScalerEnum.__members__}\")\n        self.quantile_clip = quantile_clip\n        self._flowstats_scaler_enum = flowstats_scaler_enum\n\n        if flowstats_scaler_attrs is None and flowstats_quantiles is None:\n            self.needs_fitting = True\n        elif flowstats_scaler_attrs is not None and flowstats_quantiles is not None:\n            set_scaler_attrs(scaler=self.flowstats_scaler, scaler_attrs=flowstats_scaler_attrs)\n            self.flowstats_quantiles = flowstats_quantiles\n            self.needs_fitting = False\n        else:\n            raise ValueError(\"flowstats_quantiles and scaler_attrs must be both set or both None\")\n\n    def forward(self, x_flowstats: np.ndarray) -&gt; np.ndarray:\n        if self.needs_fitting:\n            raise ValueError(\"Scalers and quantiles need to be fitted before using this transform\")\n        x_flowstats = x_flowstats.clip(min=0, max=self.flowstats_quantiles)\n        x_flowstats = self.flowstats_scaler.transform(x_flowstats) # type: ignore\n        return x_flowstats\n\n    def to_dict(self) -&gt; dict:\n        d = {\n            \"flowstats_scaler_enum\": str(self._flowstats_scaler_enum),\n            \"flowstats_scaler_attrs\": get_scaler_attrs(self.flowstats_scaler),\n            \"flowstats_quantiles\": self.flowstats_quantiles,\n            \"quantile_clip\": self.quantile_clip,\n        }\n        return d\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(flowstats_scaler={self._flowstats_scaler_enum}, quantile_clip={self.quantile_clip})\"\n</code></pre>"},{"location":"reference_transforms/#transforms.NormalizeHistograms","title":"transforms.NormalizeHistograms","text":"<p>             Bases: <code>Module</code></p> <p>Transform class for normalizing packet histograms. This class inherits from <code>nn.Module</code>, and the data transformation is implemented in the <code>forward</code> method.</p> <p>Expected format of input packet histograms: <code>(batch_size, 4 * PHIST_BIN_COUNT)</code>. The input histograms are expected to be in the following order: source packet sizes, destination packet sizes, source inter-packet times, and destination inter-packet times. Each of the four histograms is expected to have <code>PHIST_BIN_COUNT</code> bins, which is 8 in the current implementation.</p> Source code in <code>cesnet_models\\transforms.py</code> <pre><code>class NormalizeHistograms(nn.Module):\n    \"\"\"\n    Transform class for normalizing packet histograms.\n    This class inherits from `nn.Module`, and the data transformation is implemented in the `forward` method.\n\n    Expected format of input packet histograms: `(batch_size, 4 * PHIST_BIN_COUNT)`.\n    The input histograms are expected to be in the following order: source packet sizes, destination packet sizes, source inter-packet times, and destination inter-packet times.\n    Each of the four histograms is expected to have `PHIST_BIN_COUNT` bins, which is 8 in the current implementation.\n    \"\"\"\n    def __init__(self) -&gt; None:\n        super().__init__()\n\n    def forward(self, x_flowstats_phist: np.ndarray) -&gt; np.ndarray:\n        src_sizes_pkt_count = x_flowstats_phist[:, :PHIST_BIN_COUNT].sum(axis=1)[:, np.newaxis]\n        dst_sizes_pkt_count = x_flowstats_phist[:, PHIST_BIN_COUNT:(2*PHIST_BIN_COUNT)].sum(axis=1)[:, np.newaxis]\n        np.divide(x_flowstats_phist[:, :PHIST_BIN_COUNT], src_sizes_pkt_count, out=x_flowstats_phist[:, :PHIST_BIN_COUNT], where=src_sizes_pkt_count != 0)\n        np.divide(x_flowstats_phist[:, PHIST_BIN_COUNT:(2*PHIST_BIN_COUNT)], dst_sizes_pkt_count, out=x_flowstats_phist[:, PHIST_BIN_COUNT:(2*PHIST_BIN_COUNT)], where=dst_sizes_pkt_count != 0)\n        np.divide(x_flowstats_phist[:, (2*PHIST_BIN_COUNT):(3*PHIST_BIN_COUNT)], src_sizes_pkt_count - 1, out=x_flowstats_phist[:, (2*PHIST_BIN_COUNT):(3*PHIST_BIN_COUNT)], where=src_sizes_pkt_count &gt; 1)\n        np.divide(x_flowstats_phist[:, (3*PHIST_BIN_COUNT):(4*PHIST_BIN_COUNT)], dst_sizes_pkt_count - 1, out=x_flowstats_phist[:, (3*PHIST_BIN_COUNT):(4*PHIST_BIN_COUNT)], where=dst_sizes_pkt_count &gt; 1)\n        return x_flowstats_phist\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}()\"\n</code></pre>"},{"location":"reference_transforms/#enums-for-configuration","title":"Enums for configuration","text":"<p>The following enums are used for the configuration of transformations.</p>"},{"location":"reference_transforms/#transforms.ScalerEnum","title":"transforms.ScalerEnum","text":"<p>Available scalers for flow statistics, packet sizes, and inter-packet times.</p> STANDARD <code>class-attribute</code> <code>instance-attribute</code> <pre><code>STANDARD = 'standard'\n</code></pre> <p>Standardize features by removing the mean and scaling to unit variance - <code>StandardScaler</code>.</p> ROBUST <code>class-attribute</code> <code>instance-attribute</code> <pre><code>ROBUST = 'robust'\n</code></pre> <p>Robust scaling with the median and the interquartile range - <code>RobustScaler</code>.</p> MINMAX <code>class-attribute</code> <code>instance-attribute</code> <pre><code>MINMAX = 'minmax'\n</code></pre> <p>Scaling to a (0, 1) range - <code>MinMaxScaler</code>.</p>"}]}